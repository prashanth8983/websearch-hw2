\documentclass[11pt, a4paper]{report}

% --- UNIVERSAL PREAMBLE BLOCK ---
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{fontspec}

\usepackage[english, bidi=basic, provide=*]{babel}
\babelprovide[import, onchar=ids fonts]{english}

% Set default font to a clean Serif for a report style
\babelfont{rm}{Noto Serif}
\babelfont{sf}{Noto Sans} % For sans-serif if needed
\setmonofont{Noto Sans Mono} % For code listings

\usepackage{amsmath} % For math formulas like BM25
\usepackage{graphicx} % For including images
\usepackage{listings} % For formatting source code
\usepackage{xcolor} % For custom colors in listings
\usepackage{hyperref} % For clickable links in the PDF (e.g., table of contents)

% --- Customization for Code Listings ---
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.95}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}


% --- Title Page Information ---
\title{
    \Huge\bfseries Building a High-Performance Search Engine from Scratch \\
    \large A C++ Implementation of a Block-Based Inverted Index and Query Processor
}
\date{\today}


% --- Document Start ---
\begin{document}

\maketitle

\begin{abstract}
    \noindent This report provides a comprehensive technical overview of a complete information retrieval system built in C++. The system is designed to handle large text collections by constructing a compressed, block-based inverted index, which it then uses to process user queries efficiently. The project is divided into three core components: an indexer that uses the Sort-based Partial Index Memory Inversion (SPIMI) algorithm, a merger that combines partial indexes into a final, compressed structure, and a query processor that ranks documents using the Okapi BM25 algorithm. This document details the architecture, data structures, algorithms, and implementation of each component, complete with full source code listings and explanations.
\end{abstract}

\newpage
\tableofcontents
\newpage

% ==============================================================================
\chapter{System Architecture and Overview}
% ==============================================================================

\section{Introduction}
The goal of this project is to implement a complete, text-based search engine. The core challenge in any search engine is to quickly find the set of documents relevant to a user's query from a potentially vast collection. A linear scan through every document for every query is computationally infeasible. The standard solution, and the one implemented here, is an **inverted index**.

An inverted index is a data structure that maps terms (words) to the list of documents in which they appear. This "inverts" the traditional structure of documents containing terms. By pre-processing the collection to build this index, we can answer queries by looking up term lists and performing efficient set operations (intersections or unions) on them.

This project implements the entire pipeline:
\begin{enumerate}
    \item \textbf{Indexing}: Reading a large collection of documents and creating a globally sorted, compressed inverted index.
    \item \textbf{Querying}: Parsing user queries, retrieving relevant document lists from the index, and ranking them by relevance.
\end{enumerate}

\section{The Indexing Workflow: SPIMI}
When the document collection is too large to fit into RAM, we cannot build the entire index in memory at once. To solve this, we employ the **Sort-based Partial Index Memory Inversion (SPIMI)** algorithm. This is a two-phase process that allows us to build an index of arbitrary size using a fixed amount of memory.

\subsection{Phase 1: The Indexer}
The indexer reads the document collection sequentially and creates multiple, smaller, sorted index files called **runs**.
\begin{enumerate}
    \item A fixed-size memory buffer is allocated.
    \item Documents are read one by one. For each document, we extract (term, docID, frequency) tuples.
    \item These tuples are added to the buffer.
    \item When the buffer becomes full, its contents are sorted alphabetically by term, and then numerically by docID.
    \item The entire sorted buffer is written to a temporary file on disk (a "run").
    \item The buffer is cleared, and the process repeats until all documents have been processed.
\end{enumerate}

\subsection{Phase 2: The Merger}
After the indexer has finished, the disk contains multiple sorted runs. The merger's job is to combine these runs into a single, final inverted index. This is accomplished using a **k-way merge** algorithm, where 'k' is the number of runs.
\begin{enumerate}
    \item The first posting from each of the 'k' run files is read into memory.
    \item A priority queue (min-heap) is used to efficiently find the globally smallest posting (by term, then docID).
    \item The smallest posting is popped from the queue and processed.
    \item The next posting from the file that the popped posting came from is read and added to the queue.
    \item This process continues, effectively merging all sorted runs in a single pass.
\end{enumerate}

\section{Index Structure and Compression}
The final inverted index is not just a simple list. To ensure high performance and a small footprint, several optimizations are used.

\subsection{Block-Based Structure}
A term's postings list can be very long. To avoid decompressing a multi-megabyte list for a single lookup, we partition each list into fixed-size **blocks** (e.g., 128 postings per block). This allows the query processor to fetch and decompress only the specific blocks it needs.

\subsection{Compression Techniques}
\begin{itemize}
    \item \textbf{Delta Encoding}: Since docIDs in a postings list are sorted and monotonically increasing, we can store the *gaps* between them (e.g., 100, 105, 112 becomes 100, 5, 7). This results in a set of smaller integers, which are more amenable to compression.
    \item \textbf{Variable-Byte (Var-Byte) Encoding}: This is a technique to encode integers using a variable number of bytes. Smaller integers use fewer bytes. It works by using 7 bits of each byte for data and the 8th bit as a "continuation bit" to signal whether the next byte is part of the same number.
\end{itemize}

\section{Query Processing and Ranking}
The query processor uses the generated index to find and rank documents.
\begin{itemize}
    \item \textbf{Disjunctive (OR) Queries}: Retrieve all documents containing *any* of the query terms and sum their relevance scores.
    \item \textbf{Conjunctive (AND) Queries}: Retrieve only the documents containing *all* of the query terms. This is done with an efficient document-at-a-time intersection algorithm using a `nextGEQ()` function that leverages the index's block structure to skip irrelevant postings.
    \item \textbf{Ranking with Okapi BM25}: The relevance of a document to a query is not binary. The **Okapi BM25** formula is a state-of-the-art ranking function that scores documents based on term frequency (how often a term appears in a document), inverse document frequency (how rare the term is across the entire collection), and document length.
\end{itemize}

\subsection{System Workflow Diagram}
The following diagram illustrates the complete data flow, from raw documents to the final, queryable index files.
\begin{verbatim}
+-----------------------+
| collection.tsv        |  (Raw Document Data)
+-----------+-----------+
            |
            v
+-----------+-----------+
|      indexer.cpp      |
+-----------+-----------+
            |
            v
+-----------+-----------+     +-----------+-----------+
|      run_0.bin        | ... |      run_N.bin        |
+-----------------------+     +-----------------------+
            |                             |
            +-------------+---------------+
                          |
                          v
+-------------------------+-------------------------+
|                    merger.cpp                     |
+-------------------------+-------------------------+
                          |
            +-------------+---------------+
            |                             |
            v                             v
+-----------+-----------+     +-----------+-----------+
| inverted_index.bin    |     | lexicon.txt           |
| metadata.bin          |     | doc_lengths.txt       |
+-----------------------+     +-----------------------+
            |
            | (Used by...)
            v
+-----------------------+
|      query.cpp        |
+-----------------------+
\end{verbatim}

\newpage
% ==============================================================================
\chapter{Phase 1: The Indexer}
% ==============================================================================

\section{Purpose and Algorithm}
The indexer is the first stage of the pipeline, responsible for processing the raw text collection and preparing it for the final merge. As outlined in the previous chapter, it implements the in-memory portion of the SPIMI algorithm. Its sole responsibility is to produce multiple, independently sorted runs of postings. This approach effectively trades I/O operations for memory, allowing a machine with limited RAM to index a collection of nearly infinite size.

The logic revolves around a simple loop: read, tokenize, buffer, and flush. The flush step is the most critical, as it involves an in-memory sort of millions of postings, which must be highly efficient. The standard library's `std::sort` is more than capable for this task, as it is typically a highly optimized introspective sort.

\section{Key Data Structures}
\begin{itemize}
    \item \lstinline{vector<tuple<string, int, int>> postingBuffer}: The main in-memory buffer. A tuple is used to store the term, its associated docID, and its frequency within that document. The default sorting behavior of `std::tuple` is lexicographical, which is exactly what we need: it sorts by term first, then by docID, then by frequency.
    \item \lstinline{unordered_map<string, int> termFreq}: A temporary hash map used to count term frequencies for a *single* document. This is rebuilt for every document to ensure counts do not spill over.
\end{itemize}

\section{Source Code: \texttt{indexer.cpp}}
\begin{lstlisting}[language=C++, caption={The complete source code for the indexer component.}, label={lst:indexer}]
#include <iostream>
#include <fstream>
#include <sstream>
#include <string>
#include <cctype>
#include <algorithm>
#include <vector>
#include <unordered_map>
#include <cstdlib>

using namespace std;

// Tokenize a passage into terms
vector<string> tokenize(const string& s) {
    vector<string> tokens;
    string token;
    for (char c : s) {
        if (isalnum(c)) {
            token += tolower(c);
        } else if (!token.empty()) {
            tokens.push_back(token);
            token.clear();
        }
    }
    if (!token.empty()) {
        tokens.push_back(token);
    }
    return tokens;
}

// Write sorted postings to a binary run file
void writeRun(const vector<tuple<string, int, int>>& postings, int runNumber) {
    string filename = "partial/run_" + to_string(runNumber) + ".bin";
    ofstream out(filename, ios::binary);
    
    for (const auto& [term, docID, freq] : postings) {
        // Write term length and term
        int termLen = term.size();
        out.write(reinterpret_cast<const char*>(&termLen), sizeof(int));
        out.write(term.data(), termLen);
        
        // Write docID and freq
        out.write(reinterpret_cast<const char*>(&docID), sizeof(int));
        out.write(reinterpret_cast<const char*>(&freq), sizeof(int));
    }
    
    out.close();
    
    // Console log when partial index is saved
    cerr << "Saved partial index: run_" << runNumber << ".bin (" 
         << postings.size() << " postings)\n";
}

int main(int argc, char* argv[]) {
    if (argc != 2) {
        cerr << "Usage: " << argv[0] << " <input_file.tsv>\n";
        return 1;
    }

    string inputFilename = argv[1];
    ifstream file(inputFilename);
    if (!file.is_open()) {
        cerr << "Error: Cannot open file '" << inputFilename << "'\n";
        return 1;
    }

    // Buffer for postings
    vector<tuple<string, int, int>> postingBuffer;
    const size_t MAX_BUFFER_SIZE = 10000000; // 10M postings
    
    int docID = 0;
    int runNumber = 0;
    string line;
    
    // Document metadata
    ofstream pageTable("index/page_table.txt");
    ofstream docLengthFile("index/doc_lengths.txt");

    cerr << "Starting indexing...\n";

    while (getline(file, line)) {
        stringstream ss(line);
        string pid_str, passage;
        
        if (!getline(ss, pid_str, '\t')) continue;
        if (!getline(ss, passage)) continue;

        // Tokenize passage
        vector<string> tokens = tokenize(passage);
        if (tokens.empty()) continue;

        // Write to page table
        pageTable << docID << "\t" << pid_str << "\n";
        
        // Write document length
        docLengthFile << docID << "\t" << tokens.size() << "\n";

        // Count term frequencies
        unordered_map<string, int> termFreq;
        for (const auto& t : tokens) {
            termFreq[t]++;
        }

        // Add postings to buffer
        for (const auto& pair : termFreq) {
            postingBuffer.push_back({pair.first, docID, pair.second});
        }

        docID++;

        // Progress update every 100k documents
        if (docID % 100000 == 0) {
            cerr << "Indexed " << docID << " documents...\n";
        }

        // Flush buffer when full
        if (postingBuffer.size() >= MAX_BUFFER_SIZE) {
            sort(postingBuffer.begin(), postingBuffer.end());
            writeRun(postingBuffer, runNumber++);
            postingBuffer.clear();
        }
    }

    file.close();
    pageTable.close();
    docLengthFile.close();

    // Flush remaining postings
    if (!postingBuffer.empty()) {
        sort(postingBuffer.begin(), postingBuffer.end());
        writeRun(postingBuffer, runNumber++);
    }

    // Write metadata
    ofstream metaFile("index/indexer_meta.txt");
    metaFile << "total_documents\t" << docID << "\n";
    metaFile << "total_runs\t" << runNumber << "\n";
    metaFile.close();

    cerr << "\nIndexing complete!\n";
    cerr << "Total documents: " << docID << "\n";
    cerr << "Total runs: " << runNumber << "\n";

    return 0;
}
\end{lstlisting}

\section{Code Explanation}
\begin{itemize}
    \item \textbf{\texttt{tokenize()}}: A straightforward function that iterates through a string, collecting sequences of alphanumeric characters. It converts all characters to lowercase for normalization, ensuring that "Apple" and "apple" are treated as the same term.
    
    \item \textbf{\texttt{writeRun()}}: This function takes the sorted buffer and writes it to a binary file. Writing in binary is much more efficient and compact than writing text. For each posting, it writes the term's length, the term itself, the docID, and the frequency.
    
    \item \textbf{\texttt{main()}}: The main function orchestrates the process. It initializes file streams for document metadata (\lstinline{page_table.txt}, \lstinline{doc_lengths.txt}). It then enters a loop, reading the input TSV file line by line. Inside the loop, it tokenizes the document, calculates term frequencies, and adds the resulting postings to the \lstinline{postingBuffer}. When the buffer hits the \lstinline{MAX_BUFFER_SIZE} threshold, it's sorted and flushed to disk via \lstinline{writeRun()}. A final flush handles any remaining postings after the last document is read. Finally, it writes some metadata about the indexing process for the merger to use.
\end{itemize}

\newpage
% ==============================================================================
\chapter{Phase 2: The Merger}
% ==============================================================================

\section{Purpose and Algorithm}
The merger's task is to take the many sorted \lstinline{run_X.bin} files from the indexer and combine them into a single, final inverted index. It must do this efficiently, without ever loading the full content of all runs into memory. The chosen algorithm is a **k-way merge**, a classic algorithm for merging k sorted lists.

A priority queue (implemented as a min-heap) is the perfect data structure for this. The process is as follows:
\begin{enumerate}
    \item Read the first posting from each of the k runs and insert them into the priority queue. The queue is ordered by term, then by docID.
    \item In a loop, extract the minimum element from the priority queue. This element is the next posting in the globally sorted order.
    \item Process this posting by adding it to the postings list for the current term being built.
    \item Read the next posting from the same run file the extracted element came from and add it to the priority queue.
    \item When the term changes (e.g., all postings for "apple" have been processed and the next one is for "banana"), the complete postings list for "apple" is compressed and written to the final inverted index file. A corresponding entry is written to the lexicon file.
\end{enumerate}

This process streams through the run files and writes to the final index files, using only a small amount of memory to hold the priority queue (with k elements) and the postings list for the single term currently being processed.

\section{Compression and Final Output}
The merger is also responsible for compression.
\begin{itemize}
    \item \textbf{Delta Encoding}: Before writing a block of postings, the docIDs are converted from absolute values to gaps.
    \item \textbf{Var-Byte Encoding}: The delta-encoded docIDs and the original frequencies are then compressed using Var-Byte encoding to save space.
\end{itemize}

The final output consists of several files in the `index/` directory:
\begin{itemize}
    \item \lstinline{inverted_index.bin}: The compressed postings lists, stored in blocks.
    \item \lstinline{lexicon.txt}: A text file acting as the dictionary. For each term, it stores a pointer (file offset) to the start of its postings list in the main index file, along with other metadata like document frequency (DF).
    \item \lstinline{metadata.bin}: A binary file containing metadata for each block in the index (e.g., the last docID in the block, the size of the block). This is critical for the query processor to perform efficient skips.
\end{itemize}

\section{Source Code: \texttt{merger.cpp}}
\begin{lstlisting}[language=C++, caption={The complete source code for the merger component.}, label={lst:merger}]
#include <iostream>
#include <fstream>
#include <string>
#include <vector>
#include <queue>
#include <algorithm>
#include <unordered_map>

using namespace std;

const int BLOCK_SIZE = 128;

struct TermEntry {
    string term;
    int docID;
    int freq;
    int fileIndex;
    
    bool operator>(const TermEntry& other) const {
        if (term != other.term) return term > other.term;
        return docID > other.docID;
    }
};

void varbyte_encode(int num, vector<unsigned char>& output) {
    while (num >= 128) {
        output.push_back((num & 0x7F) | 0x80);
        num >>= 7;
    }
    output.push_back(num & 0x7F);
}

void writeBlock(ofstream& invFile, const vector<int>& docIDs, const vector<int>& freqs,
                vector<int>& lastDocIDs, vector<int>& docIDSizes, vector<int>& freqSizes) {
    
    // Delta encode docIDs
    vector<int> deltas;
    deltas.push_back(docIDs[0]);
    for (size_t i = 1; i < docIDs.size(); i++) {
        deltas.push_back(docIDs[i] - docIDs[i-1]);
    }
    
    // Varbyte encode
    vector<unsigned char> encodedDocIDs, encodedFreqs;
    for (int delta : deltas) {
        varbyte_encode(delta, encodedDocIDs);
    }
    for (int freq : freqs) {
        varbyte_encode(freq, encodedFreqs);
    }
    
    // Write blocks
    int docIDSize = encodedDocIDs.size();
    invFile.write(reinterpret_cast<const char*>(&docIDSize), sizeof(int));
    invFile.write(reinterpret_cast<const char*>(encodedDocIDs.data()), docIDSize);
    
    int freqSize = encodedFreqs.size();
    invFile.write(reinterpret_cast<const char*>(&freqSize), sizeof(int));
    invFile.write(reinterpret_cast<const char*>(encodedFreqs.data()), freqSize);
    
    // Store metadata
    lastDocIDs.push_back(docIDs.back());
    docIDSizes.push_back(docIDSize);
    freqSizes.push_back(freqSize);
}

// Read next posting from binary run file
bool readNextPosting(ifstream& file, string& term, int& docID, int& freq) {
    // Read term length
    int termLen;
    if (!file.read(reinterpret_cast<char*>(&termLen), sizeof(int))) {
        return false;
    }
    
    // Read term
    term.resize(termLen);
    file.read(&term[0], termLen);
    
    // Read docID and freq
    file.read(reinterpret_cast<char*>(&docID), sizeof(int));
    file.read(reinterpret_cast<char*>(&freq), sizeof(int));
    
    return true;
}

int main(int argc, char* argv[]) {
    if (argc != 2) {
        cerr << "Usage: " << argv[0] << " <num_runs>\n";
        return 1;
    }
    
    int numRuns = stoi(argv[1]);
    
    // Open all run files
    vector<ifstream> runFiles(numRuns);
    for (int i = 0; i < numRuns; i++) {
        string filename = "partial/run_" + to_string(i) + ".bin";
        runFiles[i].open(filename, ios::binary);
        if (!runFiles[i].is_open()) {
            cerr << "Error: Cannot open " << filename << "\n";
            return 1;
        }
    }
    
    // Open output files
    ofstream invFile("index/inverted_index.bin", ios::binary);
    ofstream lexFile("index/lexicon.txt");
    
    if (!invFile.is_open() || !lexFile.is_open()) {
        cerr << "Error: Cannot create output files\n";
        return 1;
    }
    
    // Priority queue for k-way merge
    priority_queue<TermEntry, vector<TermEntry>, greater<TermEntry>> pq;
    
    // Initialize
    vector<string> currentTerms(numRuns);
    vector<int> currentDocIDs(numRuns);
    vector<int> currentFreqs(numRuns);
    
    for (int i = 0; i < numRuns; i++) {
        if (readNextPosting(runFiles[i], currentTerms[i], currentDocIDs[i], currentFreqs[i])) {
            pq.push({currentTerms[i], currentDocIDs[i], currentFreqs[i], i});
        }
    }
    
    // Metadata arrays
    vector<int> allLastDocIDs, allDocIDSizes, allFreqSizes;
    
    // Statistics
    int termsProcessed = 0;
    long long totalPostings = 0;
    unordered_map<string, int> termDF;
    
    // Current term
    string currentTerm = "";
    vector<int> termDocIDs, termFreqs;
    long long termStartOffset = 0;
    int termStartBlock = 0;
    
    while (!pq.empty()) {
        TermEntry entry = pq.top();
        pq.pop();
        
        // New term - finish previous
        if (!currentTerm.empty() && entry.term != currentTerm) {
            if (!termDocIDs.empty()) {
                writeBlock(invFile, termDocIDs, termFreqs, 
                          allLastDocIDs, allDocIDSizes, allFreqSizes);
            }
            
            // Write lexicon entry
            lexFile << currentTerm << "\t" 
                    << termStartOffset << "\t"
                    << termStartBlock << "\t"
                    << totalPostings << "\t"
                    << termDF[currentTerm] << "\n";
            
            termDocIDs.clear();
            termFreqs.clear();
            termStartOffset = invFile.tellp();
            termStartBlock = allLastDocIDs.size();
            termsProcessed++;
            
            if (termsProcessed % 50000 == 0) {
                cerr << "Merged " << termsProcessed << " terms\r" << flush;
            }
        }
        
        // Process current posting
        if (entry.term != currentTerm) {
            currentTerm = entry.term;
            termDF[currentTerm] = 0;
            totalPostings = 0;
        }
        
        termDocIDs.push_back(entry.docID);
        termFreqs.push_back(entry.freq);
        termDF[currentTerm]++;
        totalPostings++;
        
        // Write block when full
        if ((int)termDocIDs.size() == BLOCK_SIZE) {
            writeBlock(invFile, termDocIDs, termFreqs,
                      allLastDocIDs, allDocIDSizes, allFreqSizes);
            termDocIDs.clear();
            termFreqs.clear();
        }
        
        // Read next from same file
        int fileIdx = entry.fileIndex;
        if (readNextPosting(runFiles[fileIdx], currentTerms[fileIdx], 
                           currentDocIDs[fileIdx], currentFreqs[fileIdx])) {
            pq.push({currentTerms[fileIdx], currentDocIDs[fileIdx], 
                    currentFreqs[fileIdx], fileIdx});
        }
    }
    
    // Process last term
    if (!currentTerm.empty()) {
        if (!termDocIDs.empty()) {
            writeBlock(invFile, termDocIDs, termFreqs,
                      allLastDocIDs, allDocIDSizes, allFreqSizes);
        }
        
        lexFile << currentTerm << "\t" 
                << termStartOffset << "\t"
                << termStartBlock << "\t"
                << totalPostings << "\t"
                << termDF[currentTerm] << "\n";
        termsProcessed++;
    }
    
    // Close files
    for (auto& f : runFiles) f.close();
    invFile.close();
    lexFile.close();
    
    // Write metadata
    ofstream metaFile("index/metadata.bin", ios::binary);
    
    int numBlocks = allLastDocIDs.size();
    metaFile.write(reinterpret_cast<const char*>(&numBlocks), sizeof(int));
    metaFile.write(reinterpret_cast<const char*>(allLastDocIDs.data()), 
                   numBlocks * sizeof(int));
    metaFile.write(reinterpret_cast<const char*>(allDocIDSizes.data()), 
                   numBlocks * sizeof(int));
    metaFile.write(reinterpret_cast<const char*>(allFreqSizes.data()), 
                   numBlocks * sizeof(int));
    metaFile.close();
    
    // Write stats
    ofstream statsFile("index/collection_stats.txt");
    statsFile << "total_terms\t" << termsProcessed << "\n";
    statsFile << "total_blocks\t" << numBlocks << "\n";
    statsFile.close();
    
    cerr << "\nMerge complete: " << termsProcessed << " terms, " << numBlocks << " blocks\n";
    
    return 0;
}
\end{lstlisting}

\section{Code Explanation}
\begin{itemize}
    \item \textbf{\texttt{TermEntry}}: A simple struct to hold a posting and the index of the run file it came from. The overloaded `>` operator is essential for the min-heap behavior of the priority queue.
    \item \textbf{\texttt{varbyte\_encode()}}: Implements Var-Byte encoding. It takes an integer and pushes one or more bytes into an output vector, setting the continuation bit on all but the last byte.
    \item \textbf{\texttt{writeBlock()}}: This function handles the compression and writing of a single block of 128 postings. It performs delta encoding, then Var-Byte encoding, and writes the compressed byte streams to the inverted index file. It also records the block's metadata (last docID, byte sizes) in vectors that will be written to `metadata.bin` at the end.
    \item \textbf{\texttt{readNextPosting()}}: A helper function to read one posting record from a binary run file.
    \item \textbf{\texttt{main()}}: The main merging logic. It initializes the priority queue by reading the first posting from all run files. The main `while (!pq.empty())` loop drives the process. It continually pulls the smallest posting, processes it, and refills the queue. The logic to detect a change in `currentTerm` is the trigger to finalize the previous term's postings list and write its entry to the lexicon. Finally, it writes out all the collected block metadata to `metadata.bin`.
\end{itemize}

\newpage
% ==============================================================================
\chapter{Phase 3: The Query Processor}
% ==============================================================================

\section{Purpose and Algorithm}
The query processor is the user-facing component of the search engine. It takes the pre-built index and uses it to answer user queries in real-time. It is responsible for parsing queries, efficiently retrieving postings from the index, and ranking the resulting documents by relevance.

\subsection{The \texttt{InvertedList} Class}
This class is the heart of the query processor. It provides an abstraction for iterating over a term's postings list without loading the entire list into memory. It reads and decompresses one block at a time, as needed.

Its most powerful method is \lstinline{nextGEQ(targetDocID)} (Next Greater Than or Equal To). This method is the key to efficient conjunctive query processing. It intelligently uses the \lstinline{lastDocIDs} metadata loaded from \lstinline{metadata.bin} to skip over entire blocks of postings that cannot possibly contain the \lstinline{targetDocID}, dramatically reducing I/O and decompression overhead.

\subsection{Ranking with Okapi BM25}
To rank documents, the system uses the Okapi BM25 formula, a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document. The formula for a single query term is:

\begin{equation}
\text{score}(D, q) = \text{IDF}(q) \cdot \frac{f(q, D) \cdot (k_1 + 1)}{f(q, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}
\end{equation}

Where:
\begin{itemize}
    \item $f(q, D)$ is the term frequency of term $q$ in document $D$.
    \item $|D|$ is the length of document $D$.
    \item $\text{avgdl}$ is the average document length in the collection.
    \item $k_1$ and $b$ are free parameters, usually chosen as $k_1 \in [1.2, 2.0]$ and $b=0.75$.
    \item $\text{IDF}(q)$ is the Inverse Document Frequency of the term, calculated as:
\end{itemize}
\begin{equation}
\text{IDF}(q) = \log\left(\frac{N - n(q) + 0.5}{n(q) + 0.5}\right)
\end{equation}
\begin{itemize}
    \item $N$ is the total number of documents in the collection.
    \item $n(q)$ is the number of documents containing the term $q$.
\end{itemize}
The total score for a document with respect to a multi-term query is the sum of the individual BM25 scores for each query term.

\section{Source Code: \texttt{query.cpp}}
\begin{lstlisting}[language=C++, caption={The complete source code for the query processor.}, label={lst:query}]
#include <iostream>
#include <fstream>
#include <sstream>
#include <string>
#include <vector>
#include <unordered_map>
#include <algorithm>
#include <cmath>
#include <cctype>

using namespace std;

const double K1 = 1.2;
const double B = 0.75;
const int BLOCK_SIZE = 128;

// Global data
unordered_map<string, tuple<long long, int, int, int>> lexicon;
vector<int> lastDocIDs, docIDSizes, freqSizes;
unordered_map<int, int> docLengths;
int totalDocuments = 0;
double avgDocLength = 0.0;

// Varbyte decoding
int varbyte_decode(const unsigned char* data, int& offset) {
    int num = 0;
    int shift = 0;
    unsigned char byte;
    
    do {
        byte = data[offset++];
        num |= (byte & 0x7F) << shift;
        shift += 7;
    } while (byte & 0x80);
    
    return num;
}

// Tokenize
vector<string> tokenize(const string& s) {
    vector<string> tokens;
    string token;
    for (char c : s) {
        if (isalnum(c)) {
            token += tolower(c);
        } else if (!token.empty()) {
            tokens.push_back(token);
            token.clear();
        }
    }
    if (!token.empty()) {
        tokens.push_back(token);
    }
    return tokens;
}

// Inverted List API
class InvertedList {
private:
    ifstream& invFile;
    string term;
    long long startOffset;
    int startBlock;
    int numPostings;
    
    int currentBlockIdx;
    vector<int> currentDocIDs;
    vector<int> currentFreqs;
    int positionInBlock;
    bool finished;
    
    void decompressBlock(int blockIdx) {
        currentDocIDs.clear();
        currentFreqs.clear();
        
        if (blockIdx >= startBlock + (numPostings + BLOCK_SIZE - 1) / BLOCK_SIZE) {
            finished = true;
            return;
        }
        
        // Calculate file position
        long long offset = startOffset;
        for (int i = startBlock; i < blockIdx; i++) {
            offset += sizeof(int) + docIDSizes[i] + sizeof(int) + freqSizes[i];
        }
        
        invFile.seekg(offset);
        
        // Read docID block
        int docIDSize;
        invFile.read(reinterpret_cast<char*>(&docIDSize), sizeof(int));
        
        vector<unsigned char> docIDBlock(docIDSize);
        invFile.read(reinterpret_cast<char*>(docIDBlock.data()), docIDSize);
        
        // Read freq block
        int freqSize;
        invFile.read(reinterpret_cast<char*>(&freqSize), sizeof(int));
        
        vector<unsigned char> freqBlock(freqSize);
        invFile.read(reinterpret_cast<char*>(freqBlock.data()), freqSize);
        
        // Decode docIDs
        int offset_docID = 0;
        while (offset_docID < docIDSize) {
            currentDocIDs.push_back(varbyte_decode(docIDBlock.data(), offset_docID));
        }
        
        // Convert deltas to absolute
        for (size_t i = 1; i < currentDocIDs.size(); i++) {
            currentDocIDs[i] += currentDocIDs[i-1];
        }
        
        // Decode frequencies
        int offset_freq = 0;
        while (offset_freq < freqSize) {
            currentFreqs.push_back(varbyte_decode(freqBlock.data(), offset_freq));
        }
        
        positionInBlock = 0;
    }
    
public:
    InvertedList(ifstream& file, const string& t) 
        : invFile(file), term(t), finished(false) {
        
        auto it = lexicon.find(term);
        if (it == lexicon.end()) {
            finished = true;
            return;
        }
        
        startOffset = get<0>(it->second);
        startBlock = get<1>(it->second);
        numPostings = get<2>(it->second);
        
        currentBlockIdx = startBlock;
        decompressBlock(currentBlockIdx);
    }
    
    bool nextGEQ(int targetDocID) {
        if (finished) return false;
        
        // Skip blocks using metadata
        while (currentBlockIdx < startBlock + (numPostings + BLOCK_SIZE - 1) / BLOCK_SIZE) {
            if (lastDocIDs[currentBlockIdx] >= targetDocID) {
                if (currentDocIDs.empty() || positionInBlock >= (int)currentDocIDs.size()) {
                    decompressBlock(currentBlockIdx);
                }
                break;
            }
            currentBlockIdx++;
        }
        
        if (finished) return false;
        
        // Find within block
        while (positionInBlock < (int)currentDocIDs.size()) {
            if (currentDocIDs[positionInBlock] >= targetDocID) {
                return true;
            }
            positionInBlock++;
        }
        
        // Move to next block
        currentBlockIdx++;
        if (currentBlockIdx >= startBlock + (numPostings + BLOCK_SIZE - 1) / BLOCK_SIZE) {
            finished = true;
            return false;
        }
        
        decompressBlock(currentBlockIdx);
        return nextGEQ(targetDocID);
    }
    
    bool hasNext() {
        return !finished && positionInBlock < (int)currentDocIDs.size();
    }
    
    int getDocID() {
        if (!hasNext()) return -1;
        return currentDocIDs[positionInBlock];
    }
    
    int getFrequency() {
        if (!hasNext()) return 0;
        return currentFreqs[positionInBlock];
    }
    
    void next() {
        positionInBlock++;
        if (positionInBlock >= (int)currentDocIDs.size()) {
            currentBlockIdx++;
            if (currentBlockIdx < startBlock + (numPostings + BLOCK_SIZE - 1) / BLOCK_SIZE) {
                decompressBlock(currentBlockIdx);
            } else {
                finished = true;
            }
        }
    }
};

// BM25 score
double calculateBM25(int tf, int docLength, int df, int N) {
    double idf = log((N - df + 0.5) / (df + 0.5));
    double tfComponent = (tf * (K1 + 1.0)) / (tf + K1 * (1.0 - B + B * (docLength / avgDocLength)));
    return idf * tfComponent;
}

// Disjunctive query
vector<pair<int, double>> processDisjunctiveQuery(ifstream& invFile, const vector<string>& queryTerms) {
    unordered_map<int, double> docScores;
    
    for (const auto& term : queryTerms) {
        auto it = lexicon.find(term);
        if (it == lexicon.end()) continue;
        
        int df = get<3>(it->second);
        InvertedList list(invFile, term);
        
        list.nextGEQ(0);
        while (list.hasNext()) {
            int docID = list.getDocID();
            int freq = list.getFrequency();
            int docLen = docLengths.count(docID) ? docLengths[docID] : (int)avgDocLength;
            
            double score = calculateBM25(freq, docLen, df, totalDocuments);
            docScores[docID] += score;
            
            list.next();
        }
    }
    
    vector<pair<int, double>> results;
    for (const auto& pair : docScores) {
        results.push_back({pair.first, pair.second});
    }
    
    sort(results.begin(), results.end(), [](const auto& a, const auto& b) {
        return a.second > b.second;
    });
    
    return results;
}

// Conjunctive query
vector<pair<int, double>> processConjunctiveQuery(ifstream& invFile, const vector<string>& queryTerms) {
    if (queryTerms.empty()) return {};
    
    vector<InvertedList*> lists;
    vector<int> dfs;
    
    for (const auto& term : queryTerms) {
        auto it = lexicon.find(term);
        if (it == lexicon.end()) {
            for (auto* list : lists) delete list;
            return {};
        }
        
        dfs.push_back(get<3>(it->second));
        lists.push_back(new InvertedList(invFile, term));
    }
    
    unordered_map<int, double> docScores;
    
    lists[0]->nextGEQ(0);
    while (lists[0]->hasNext()) {
        int docID = lists[0]->getDocID();
        bool inAll = true;
        
        vector<int> freqs = {lists[0]->getFrequency()};
        
        for (size_t i = 1; i < lists.size(); i++) {
            lists[i]->nextGEQ(docID);
            if (!lists[i]->hasNext() || lists[i]->getDocID() != docID) {
                inAll = false;
                break;
            }
            freqs.push_back(lists[i]->getFrequency());
        }
        
        if (inAll) {
            int docLen = docLengths.count(docID) ? docLengths[docID] : (int)avgDocLength;
            double totalScore = 0.0;
            for (size_t i = 0; i < queryTerms.size(); i++) {
                totalScore += calculateBM25(freqs[i], docLen, dfs[i], totalDocuments);
            }
            docScores[docID] = totalScore;
        }
        
        lists[0]->next();
    }
    
    for (auto* list : lists) delete list;
    
    vector<pair<int, double>> results;
    for (const auto& pair : docScores) {
        results.push_back({pair.first, pair.second});
    }
    
    sort(results.begin(), results.end(), [](const auto& a, const auto& b) {
        return a.second > b.second;
    });
    
    return results;
}

// Load index
bool loadIndex() {
    ifstream lexFile("index/lexicon.txt");
    if (!lexFile.is_open()) return false;
    
    string line;
    while (getline(lexFile, line)) {
        stringstream ss(line);
        string term;
        long long offset;
        int startBlock, numPostings, df;
        ss >> term >> offset >> startBlock >> numPostings >> df;
        lexicon[term] = make_tuple(offset, startBlock, numPostings, df);
    }
    lexFile.close();
    
    ifstream metaFile("index/metadata.bin", ios::binary);
    if (!metaFile.is_open()) return false;
    
    int numBlocks;
    metaFile.read(reinterpret_cast<char*>(&numBlocks), sizeof(int));
    
    lastDocIDs.resize(numBlocks);
    docIDSizes.resize(numBlocks);
    freqSizes.resize(numBlocks);
    
    metaFile.read(reinterpret_cast<char*>(lastDocIDs.data()), numBlocks * sizeof(int));
    metaFile.read(reinterpret_cast<char*>(docIDSizes.data()), numBlocks * sizeof(int));
    metaFile.read(reinterpret_cast<char*>(freqSizes.data()), numBlocks * sizeof(int));
    metaFile.close();
    
    ifstream docLenFile("index/doc_lengths.txt");
    if (docLenFile.is_open()) {
        while (getline(docLenFile, line)) {
            stringstream ss(line);
            int docID, length;
            ss >> docID >> length;
            docLengths[docID] = length;
            totalDocuments++;
            avgDocLength += length;
        }
        docLenFile.close();
        avgDocLength /= totalDocuments;
    }
    
    return true;
}

int main() {
    if (!loadIndex()) {
        cerr << "Error loading index\n";
        return 1;
    }
    
    cout << "Search engine ready. Type 'quit' to exit.\n";
    cout << "Prefix queries with 'AND:' for conjunctive, 'OR:' for disjunctive (default).\n\n";
    
    ifstream invFile("index/inverted_index.bin", ios::binary);
    if (!invFile.is_open()) {
        cerr << "Error opening inverted index\n";
        return 1;
    }
    
    string line;
    while (true) {
        cout << "Query> ";
        if (!getline(cin, line)) break;
        
        line.erase(0, line.find_first_not_of(" \t\n\r"));
        line.erase(line.find_last_not_of(" \t\n\r") + 1);
        
        if (line.empty()) continue;
        if (line == "quit" || line == "exit") break;
        
        bool conjunctive = false;
        string query = line;
        
        if (line.size() >= 4 && line.substr(0, 4) == "AND:") {
            conjunctive = true;
            query = line.substr(4);
        } else if (line.size() >= 3 && line.substr(0, 3) == "OR:") {
            query = line.substr(3);
        }
        
        vector<string> queryTerms = tokenize(query);
        if (queryTerms.empty()) continue;
        
        vector<pair<int, double>> results;
        if (conjunctive) {
            results = processConjunctiveQuery(invFile, queryTerms);
        } else {
            results = processDisjunctiveQuery(invFile, queryTerms);
        }
        
        cout << "\nTop 10 results:\n";
        for (int i = 0; i < min(10, (int)results.size()); i++) {
            cout << (i + 1) << ". DocID " << results[i].first 
                 << " (score: " << results[i].second << ")\n";
        }
        cout << "Total: " << results.size() << " documents\n\n";
    }
    
    invFile.close();
    return 0;
}
\end{lstlisting}

\section{Code Explanation}
\begin{itemize}
    \item \textbf{\texttt{loadIndex()}}: This function runs once at startup. It reads the lexicon, document lengths, and block metadata into global in-memory maps and vectors for fast access during query time.
    \item \textbf{\texttt{varbyte\_decode()}}: The inverse of the encoder in the merger. It reads a stream of bytes and reconstructs the original integer.
    \item \textbf{\lstinline{InvertedList::decompressBlock()}}: This private method handles the on-demand reading and decompression of a single block from the \lstinline{inverted_index.bin} file. It reads the Var-Byte encoded data, decodes it, and reconstructs the absolute docIDs from the deltas.
    \item \textbf{\texttt{processDisjunctiveQuery()}}: Implements OR query logic. It iterates through each query term, retrieves its full postings list, calculates the BM25 score for each document in the list, and accumulates the scores in a hash map.
    \item \textbf{\texttt{processConjunctiveQuery()}}: Implements AND query logic. This is more complex. It opens an \lstinline{InvertedList} for each query term and uses a document-at-a-time approach. It uses the \lstinline{docID} from the first list as a "pivot" and calls \lstinline{nextGEQ()} on all other lists to efficiently seek to that pivot. If all lists contain the same \lstinline{docID}, it is a match, and its score is calculated. If not, the pivot is advanced to the highest \lstinline{docID} seen, and the process repeats.
    \item \textbf{\texttt{main()}}: The main interactive loop. It loads the index, accepts queries, determines the query type (AND/OR), calls the appropriate processing function, and prints the top 10 ranked results.
\end{itemize}

\chapter{Conclusion}
This project successfully demonstrates the end-to-end construction of a modern search engine. By implementing the SPIMI algorithm, the system is capable of indexing document collections that far exceed available system memory. The use of a k-way merge, combined with delta and Var-Byte compression, produces a final index that is both compact and structured for high-performance retrieval. The query processor leverages this structure, particularly through block-skipping, to answer conjunctive and disjunctive queries efficiently, ranking results with the industry-standard BM25 algorithm. While this implementation forms a strong foundation, future work could include adding support for phrase queries, implementing more advanced text normalization (stemming and stop-word removal), and distributing the index across multiple machines for even greater scalability.


\end{document}

